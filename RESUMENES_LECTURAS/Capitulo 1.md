# The Golem of Prague
En el siglo 16 la casa de Habsburg la cual controlaba la mitad de Europa, el emperador dueño era Rudolph segundo, él cual apreciaba mucho la educación en Praga. Creo un robot de arcilla el cual era conocido como Golem en el folclor judío, cobrando vida con una palabra el cual obedecía las ordenes, siendo un peligro para las ordenes mal dadas. Este Golem los protegía de las persecuciones, pero por las ordenes previstas causaron muchas muertes de vidas inocentes que como en consecuencia terminaron en su destrucción.

## Golem estadístico
Los golem estadísticos no poseen forma física, se conocen como los modelos científicos, los cuales tienen efectos en las predicciones que calculan con un propósito. Se podría relacionar con el Golem debido a que una mala predicción conllevaría a un daño.
![[Pasted image 20251112211455.png]]
Figura 1. Es un ejemplo de un arbol de decisión para seleccionar el modelo estadístico más apropiado siguiendo las condiciones que responde.
Cada vez que se usa un modelo estadístico se esta utilizando un Gólem que realizara los análisis propuestos, pero como se ve en el arbol existe más de un modelo dependiendo de los parámetros.
La estadística se puede definir como una rama de la ingeniería que con un conjunto de principios y restricciones deriva en una gran diversidad de aplicaciones. 
Los modelos estadísticos con pruebas diseñadas con un propósito especifico, que al responder una serie de preguntas se elige el más adecuado; los investigadores usualmente usan los modelos sin comprenderlos ya que existen diversos maneras de aplicarlos y elegir el adecuado es un desafió.
El principal problema es que las herramientas tradicionales no son lo bastante diversas para responder varias preguntas comunes, y ninguna resuelve el problema de la inferencia de causas a partir de la evidencia, no entienden la causalidad, solo la asociación.
Se necesita una teoría unificada con los principios de diseñar, construir y perfeccionar los procedimientos estadísticos, siendo la estadística un conjunto de estrategias. 

## Pensamiento estadístico
Si no se entiende como el modelo realiza el procesamiento de la información, no se pueden interpretar de manera correcta los resultados. 
Esto requiere de un conocimiento detallado del modelo y un calculo manual, para usar las soluciones de manera automática. Tambien esta la forma en que los conceptos están definidos los objetivos estadísticos y la interpretación de los resultados; se necesita cierta [[epistemología]] estadística, una comprensión de la relación de los modelos con las hipótesis. 
Karl Popper (1902-1994) argumento que la ciencia avanza para falsificar al hipótesis, ya que la ciencia funciona mejor al usar este principio; este pensamiento es de un filosofo informal de la ciencia la cual es común entre científicos pero no entre filósofos, debido a que la ciencia no describe la falsación deductiva ya que es imposible. 
Existen dos razones: 
1. Hipótesis no son modelos. Muchos modelos pueden corresponder a la misma hipótesis y muchas hipótesis a un solo modelo, haciendo imposible la falsación.
2. La medición importa, ya que muchos observadores no confiaran en los datos. 
==El método científico no puede reducirse a un procedimiento estadístico.== La falsación se aplica a algo diferente a un modelo explicativo, false una hipótesis nula no la hipótesis de investigación. 

### Hipótesis no son modelos
Cuando el intento no es estadístico existe un modelo de medición para evidenciar la hipótesis. 
No es posible deducir que una hipótesis es falsa porque se rechaza un modelo que deriva de ella. 
![[Pasted image 20251112220330.png]]
Figura 2. Relaciones entre las hipótesis.
Un ejemplo es el debate por la evolución neutral, según la figura 2 a a la izquierda están las hipótesis estereotipadas, con la primera de evolución neutral como hipótesis nula y la alternativa por selección natural; en la mitad están los modelos de proceso con la primera supone una estructura población contante en el tiempo y el otro fluctúa con el tiempo.
La dirección del tiempo en un modelo implica que unos eventos son la causa de otros, pero nunca a la inversa. 

Un modelo estadístico expresa las asociaciones entre variables más no las relaciones causales especificas. Para obtener un modelo estadístico a partir de uno causal se debe derivar la distribución de la frecuencia que se espera.
En el ejemplo que observa que un mismo modelo estadístico puede corresponder a más de un modelo de proceso, al igual que la hipótesis puede corresponder a más de un modelo y viceversa.
Si todos los modelos realizan predicciones similares, se debe buscar una descripción diferente de evidencia, bajo la cual los procesos se vean distintos. 
Los modelos estadísticos pueden corresponder a muchos modelos que se basan en distribuciones pertenecientes a la familia exponencial, en la cual se alinean con la entropía para realizar un trabajo útil. 
### Importancia de la medición: 
En la medición se observa una refutación lógica con la hipótesis nula se busca la demostración por la observación, al no encontrarla la hipótesis nula es rechazada. Este razonamiento es conocido como "método de destrucción" siendo en latín "modus tollens". De igual manera al encontrar la observación no dice nada con certeza de la hipótesis, ya que otras podrían predecir esta observación.
Una fábula explica esto en manera de la coloración del plumaje de los cisnes ya que inicialmente se decía que todos los cisnes eran blancos (H0), pero los europeos al llegar a Australia encontraron cisnes negros (rechazando H0).
Como clave diciendo que una sola observación basto para rechazar la hipótesis nula, pero encontrar evidencia tan clara como la del cisne no es algo que pase siempre y la correspondencia entre las hipótesis con los modelos e uno de los problemas que se pueden presentar; sin embargo hay dos principales:
1. Observaciones propensas a errores.
2. Hipótesis cuantitativas, poseen grados de existencia (presencia o ausencia).
#### 1. Observaciones
Poca seguridad de haber detectado el resultado que refute las hipótesis, presencia de valores intermedios, y muchas otras dificultades se presentan. Lo cual se conoce como un fenómeno hipotético que suele estar tan cuestionado como el fenómeno mismo.
Encontrar casos que refuten la hipótesis se complica por las dificultades de observación.
Existen confirmaciones erróneas (falsos positivos) y refutaciones erróneas (falsos negativos).
Debido a la limitación unida a la medición Popper consideraba que la ciencia era más amplia que la falsificación. 

#### Hipótesis continuas
Se busca estimar y explicar la distribución con la mayor precisión.
No existe la definición de una buena hipótesis, la mayoría coincide que diseñar experimentos y observaciones ayuda a la diferenciación de la hipótesis contrapuestas, siendo comparación de grado y no de tipo.


### La falsificación es consensual
La falsación siempre es consensual, no lógica. En los debas científicos sobre esto son muy complejos provocando la tergiversión de la información diciendo que esta es lógica.

## Herramientas para la ingeniería de Gólems
Modelamos para no imitar la falsación. Estos modelos pueden convertirse en procedimientos de prueba, para diseñar, pronosticar y argumentar. 
La investigación se beneficia de la capacidad de producir y manipular los modelos, para generar un modelo estadístico necesita un modelo generativo.
Algunas herramientas relacionadas entre sí para los propósitos son:
1. Análisis de datos Bayesiano
2. Comparación de modelos
3. Modelos multinivel
4. Modelos causales gráficos
Para su compresión se debe implementar.

### Análisis de datos Bayesiano
Existen muchos enfoque ya sean heurísticos como formales para comprender el mundo por medio de los datos. 
Este análisis toma una pregunta en forma de modelo y con la lógica genera una respuesta en forma de distribuciones de probabilidad; cuenta el número de maneras que lo datos pueden ocurrir, según las suposiciones. 
Los eventos con más maneras son plausibles, ya que se utiliza l a teoria de probabilidad como una manera de representar la plausibilidad para eventos contables o parametros; el resto se debe deducir logicamente. 
La probabilidad bayesiana es un enfoque muy general que incluye el enfoque frecuentista. El enfoque frecuentista requiere la definición de las probabilidades en función de la frecuencia de los eventos en muestras grandes, basándose en un re muestreo hipotético de los datos; significando que solo las mediciones pueden tener una distribución de probabilidad no los parámetros y los modelos. 
La distribución de las mediciones se denomina distribución muestral.

La distribución muestral de cualquier medición es constante porque la medición es determinista, sin aleatoriedad; siendo difícil aplicar la estadística frecuentista. La inferencia bayesiana procede ya que el ruido determinista se puede modelar por probabilidad, sin confundí la probabilidad como frecuencia. La reconstrucción y procesamiento de imágenes esta dominado por algoritmos bayesianos.
En ciertos procedimientos estadísticos la diferencia en los conceptos de probabilidad tiene poco impacto, sin embargo así el frecuentista y el bayesiano tengan el mismo resultado no justifican sis inferencias con un supuesto re muestreo.
El bayesiano considera la aleatoriedad como propiedad de la información, pero nada en el mundo real es aleatorio. Usa la aleatoriedad para describir la incertidumbre ante un conocimiento incompleto. 
==En un algoritmo el lanzamiento de la moneda es aleatorio, pero es el algoritmo el que es aleatorio, no la moneda.==
La probabilidad no es unitaria, existe más de una definición de esta, ya que los axiomas están sujetos a un debate en la interpretación, existiendo diferentes versiones de la probabilidad bayesiana (el libro sigue la de Cox/ Laplace Jeffreys-Cox Jaynes).

Existen otros enfoques pero el bayesiano tiene una ventaja pedagógica ya que resulta más intuitivo. Ya que muchos científicos interpretan resultados no bayesianos con bayesianos.
Las intuiciones de los científicos estarán menos en desacuerdo con la lógica del marco teórico, debido a la poca interpretación de una probabilidad como un valor p.

En la historia el enfoque bayesiano es mas antiguo que otros enfoques, ya que se aplicao a finales del siglo XVIII y en el siglo XIX. Pero debido a Fisher se fue usando poco, ya que decía basarse en un error. Y debido a los nuevos enfoques computacionales se impulso nuevamente el uso del enfoque bayesiano a mitad del siglo XX. 

### Comparación  predicción de modelos
El método bayesiano permite que los modelos puedan aprender de los datos. Para la elección de un modelo se hacen con los que hacen mejores predicciones y para esto se hace uso de dos herramientas para conocer el futuro: 
- La validación cruzada
- Los criterios de información
Estas herramientas comparan modelos basándose en la precisión predictiva que se espera. Lo cual conduce a descubrir que los modelos complejos hacen peores predicciones que los simples. 
La paradoja es que ajustar es fácil pero predecir no.
Los modelos complejos se sobre ajustan mas que los simples, por lo que la validación cruzada y los criterios de información dan tres maneras de relacionar:
1. Expectativas útiles sobre la precisión en la predicción.
2. Estimación de la tendencia de un modelo a sobre ajustarse a los datos. Entiendo la interacción entre modelos y datos.
3. La validación cruzada y los criterios de información ayudan a identificar las observaciones influyentes.
### Modelos de multinivel
Los modelos estadísticos contienen parámetros que permiten realiza las inferencias. De manera que un parámetro puede considerarse un marcador de posición para un modelo. Al tener un modelo donde el parámetro muestra como obtiene su valor, se puede integrar un nuevo modelo dentro del anterior, creando así modelos de multiples niveles de incertidumbre donde cada uno alimenta al siguiente.
Estos modelos también se conocen como ==jerárquicos==, con efectos aleatorios; dado que tienen una representación bayesiana se han desarrollado de manera paralela a el análisis de datos bayesiano.
Ayudan a lidiar con el sobre ajuste, realizando un agrupación parcial, la cual combina información de diferentes unidades para producir mejores estimaciones en todas las unidades.
Se analizan 4 ejemplos más comunes que solo aplican cuando el investigador reconoce a los grupos: 
(1) Para ajustar las estimaciones ante muestreos repetidos. Cuando se obtiene más de una observación de una misma variable, los modelos tradicionales de un solo nivel pueden resultar inexactos. 
(2) Para ajustar las estimaciones ante desequilibrios en el muestreo. Cuando algunos individuos, lugares o momentos se muestrean con mayor frecuencia que otros, los modelos de un solo nivel pueden llevarnos a conclusiones erróneas. 
(3) Para estudiar la variación. Si nuestras preguntas de investigación incluyen la variación entre individuos u otros grupos dentro de los datos, los modelos multinivel son de gran utilidad, ya que modelan la variación de forma explícita. 
(4) Para evitar el promedio. Con frecuencia, los investigadores promedian previamente algunos datos para construir variables para un análisis de regresión. Esto puede ser dañino, ya que el promedio elimina la variación y, genera una falsa sensación de seguridad. Los modelos multinivel permiten preservar la incertidumbre en los valores originales, antes del promedio, al tiempo que utilizamos el promedio para realizar predicciones.

Cada grupo puede tener una tendencia promedio diferente o responder diferente, los agrupados se benefician de modelarlo por un algoritmo que predice la variación.
Muchos tipos de modelos son multinivel:
- Modelos para datos faltantes
- Error de medición
- Análisis factorial
- Modelos de series temporales (algunos)
- Tipos de regresión espacial y de red
- Regresiones filogenéticas
La regresión multinivel debería ser la forma predeterminada de regresión. De manera que los modelos que no la usan deben justificar porque, ya que generan una variación que los modelos multinivel intentan cuantificar y que unidades de los datos respondieron.
Ajustar e interpretar modelos multinivel puede ser más difícil que hacerlo en un modelo tradicional.

### Modelos causales gráficos
Un modelo estadistico es una herramienta de asociación, que permite detectar la asociación causa y sus efectos. Pero necesita de datos externos para determinar que explicación es la correcta.
La validación cruzada y los criterios de información se describió el sobre ajuste como la principal paradoja de la predicción. Pero ahora la paradoja es que los modelos causales incorrectos pueden realizar mejores predicciones que los correctos.
Los experimentos controlados aleatorizados permiten inferencia causal, al igual que los aleatorizados la permiten. La predicción se puede intentar adivinar con validación cruzada pero sugieren modelos con variables de confusión las cuales son asociaciones reales crean relaciones causales incorrectas; pero pueden mejorar la predicción. 
Para buenas predicciones, en ausencia de intervención se necesita una buena comprensión causal. 

Pero se necesita un modelo causal que pueda usarse para diseñar uno o mas modelos para la identificación de estas relaciones causales. Un modelo científico completo tiene mas información que un modelo estadístico derivado de el. 
Estos métodos formales son de la primera mitad del siglo XX, extendiendo al estudio de la medición, el diseño experimental y la capacidad de generalizar o transferir resultados entre muestras. 

Cuando no se dispone de un modelo causal completo, sino solo de uno heurístico que indica qué variables influyen causalmente en otras, aún se pueden utilizar estas herramientas lógicas

El modelo causal gráfico más simple es un grafo acíclico dirigido (DAG). Los DAG son heurísticos; no son modelos estadísticos detallados. El DAG depende de la información externa a los datos para diseñar la estructura y los modelos. 
No hay modelo que acepte los datos sin procesar y devuelva un modelo fiable de las relaciones causales; la inferencia causal depende de un modelo causal independiente del modelo estadístico; pero depende de las condiciones de aleatoriedad y control experimental. 
La ensalada causal consiste en la introducción de variables de control en el modelo estadístico, para los cambios en las estimaciones y explicar la causalidad.
Un modelo que realice buenas predicciones puede equivocarse en la causalidad. 